\section{实验结果与分析}

在这一章将列出并行加载数据、不同并行方式下实验的耗时结果。使用python的time库记录耗时，并对结果进行探讨和分析。
数据集的规模是60000张图片，其余参数按照第二章第三节中进行设置，且在各个实验中始终保持一致。

在本次实验中，加速比的计算公式为：
\begin{equation}
    S_p=\frac{T_1}{T_p}
\end{equation}
$p$表示处理器数量或进程数量，$T_1$表示程序顺序执行的时间，$T_p$表示当有$p$个处理器或进程时程序的执行时间。
当$S_p=p$时，便可称为线性加速比。当某一并行算法的加速比为线性加速比时，若将处理器数量加倍，执行速度也会加倍。

并行效率的计算公式为：
\begin{equation}
    E_p=\frac{S_p}{p}=\frac{T_1}{pT_p}
\end{equation}

并行效率用于表示在解决问题时，相较于在通信与同步上的花费，
参与计算的处理器获取成分利用的程度。由此可见，拥有线性加速比的算法并行效率为1，$E_p$的值介于$0\sim 1$之间。

\subsection{并行加载数据实验}

为计算加速比，首先得到不使用多进程加载数据的时间为0.00044秒。
依次更改进程数量，观察耗时与计算加速比，结果如表\ref{tab:multi-pro}所示。
\begin{table}[h]
    \centering
    \caption{多进程加载数据的耗时与加速比}
    \begin{tabular}{>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{10em}>{\centering\arraybackslash}p{3em}>{\raggedleft\arraybackslash}p{5em}}
    \toprule
    进程数量 & 时间（单位：秒） & 加速比 & 并行效率 \\ \midrule
    2 & 0.00081 & 0.54 & 27\% \\ 
    8 & 0.00056 & 0.78 & 9.8\% \\
    16 & 0.00044 & 1.0 & 6.25\% \\
    20 & 0.00042 & 1.04 & 5.23\% \\
    32 & 0.00037 & 1.18 & 3.71\% \\
    50 & 0.00027 & 1.62 & 3.25\% \\
    60 & 0.00049 & 0.89 & 1.49\% \\
    64 & 0.00126 & 0.35 & 0.5\% \\
    \bottomrule
    \end{tabular}
    \label{tab:multi-pro}
\end{table}

如表\ref{tab:multi-pro}所示，在20核心40线程的处理器中，进程数量和处理器线程数近似相等时会取得较为良好的加速比。
\begin{enumerate}
    \item 当进程数量很少时，处理器空载，系统需要维护任务的调度和进程通信的开销，所以加速比小于1，效果反而不如单进程；
    \item 当进程数量和处理器线程数近似时，处理器满载而不是空载或负载，所以取得的加速比效果较为良好，但并行效率较低；
    \item 当进程远远大于处理器线程数时，处理器负载，需要更大的内存和更多的处理器负担来维护任务调度和进程通信的开销，
    加速比反而下降，并行效率也显著降低。
\end{enumerate}
所以要合理的设置任务中进程的数量，尽量和处理器的线程数相等。

\subsection{不同并行方式加速比对比}

按照第二章的实验参数，进行不同数据并行方法的实验：DataParallel和DistributedDataParallel并行方法对比。
因为模型参数过多，所以不进行CPU实验和单个GPU实验。
考虑到有多人在同一时刻使用服务器，防止影响他人正常工作，所以不进行多机实验，只进行单机多卡下不同并行方法的对比实验。

防止用户端持续等待程序结束，所以使用nohup挂起执行程序。并将结果重定向输出到log日志中，程序结束后即可查看执行时间。
DataParallel消耗时间$t_1=137172$秒，DistributedDataParallel消耗时间$t_2=89856$秒。
由此可见，DistributedDataParallel方法优于DataParallel方法，加速比为$t_1/t_2=1.53$。

此外，如果只实现了模型加速而忽略了模型的准确率，那么并行加速就失去了意义。即并行加速的同时
需要考虑结果的准确性，因此需要对比两种并行加速方法的准确率。
DataParallel的准确率为0.566，DistributedDataParallel的准确率为0.582，准确率结果在
可接受的误差范围内。

\subsection{更改实验参数后的结果与分析}

考虑到DistributedDataParallel方法的通信模式会受到数据量大小batchsz的影响，所以考虑更改batchsz的大小
进而分析实验结论。为防止浪费计算资源，以batchsz为30的条件下，DistributedDataParallel执行时间为阈值。
当程序执行时间大于阈值后会自动中断。

\begin{enumerate}
    \item 将batchsz的大小由30改为2，即一次训练的数据量是之前的$\frac{1}{15}$。
    多次执行DistributedDataParallel程序，程序因执行时间大于阈值而被强行中断。
    由此可见，AllReduce在数据量很小的情况下，每一次梯度更新带来的进程通信、
    任务调度与分配的开销占比会增大，而用于数据通信的开销占比却很小，导致并行效率没有明显提升。
    因此AllReduce不适合数据量较小情况下的通信。
    \item 而后将batchsz的大小由30改为60，即一次训练的数据量是之前的2倍。
    多次执行DistributedDataParallel程序，得到的程序执行时间为73824秒。
    由此可见，当数据量较大时数据通信的占比也会随之增大，并行效率相对之前也有所提升。
    所以AllReduce更适合数据量较大情况下的通信。
\end{enumerate}
